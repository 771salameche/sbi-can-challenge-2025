import logging
import sys
from pathlib import Path

# Add the 'src' directory to the Python path
sys.path.insert(0, str(Path(__file__).resolve().parent / "src"))

from src.ingestion import scraper, processor, loader
from src import config # Import config from src

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(Path(__file__).resolve().parent / "logs" / "ingestion.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def main():
    """
    Main function to run the complete data ingestion and processing pipeline.
    This script orchestrates the scraping, processing, and vectorization of data
    to build the knowledge base for the RAG application.
    """
    logger.info("Starting the data ingestion pipeline...")

    # Ensure all necessary environment variables are set before starting.
    try:
        config.check_environment_variables()
    except ValueError as e:
        logger.error(f"Configuration error: {e}")
        sys.exit(1) # Exit if configuration is missing

    # Ensure log directory exists
    log_dir = Path(__file__).resolve().parent / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # --- Step 1: Scraping ---
    logger.info(f"Step 1: Scraping new data to {config.RAW_DATA_PATH}...")
    scraper.scrape_le360(
        main_url="https://sport.le360.ma/football/can",
        output_filepath=config.RAW_DATA_PATH / "le360_can_articles.json"
    )
    scraper.scrape_sofascore(output_dir=config.RAW_DATA_PATH)
    scraper.scrape_transfermarkt(output_dir=config.RAW_DATA_PATH)
    scraper.scrape_wikipedia(output_dir=config.RAW_DATA_PATH)
    logger.info("Scraping complete.")

    # --- Step 2: Data Processing ---
    logger.info(f"Step 2: Cleaning, structuring, and merging data to {config.PROCESSED_DATA_PATH}...")
    
    # Process Le360 articles
    processor.process_le360_articles_json_to_rag(
        input_filepath=config.RAW_DATA_PATH / "le360_can_articles.json",
        output_filepath=config.PROCESSED_DATA_PATH / "le360_can_articles_rag.txt"
    )
    # Process Le360 details (requires a specific JSON structure, assume it's created or manually placed)
    # NOTE: The original `extract_le360_details.py` implied a 'temp_le360_data.json'
    # For a fully automated pipeline, ensure this file is generated by a scraper or other means.
    # For now, we'll use a placeholder or assume it's pre-existing.
    # processor.process_le360_details(
    #     input_json_filepath=dummy_le360_details_json_path,
    #     output_filepath=config.PROCESSED_DATA_PATH / "le360_can2025_details_fr.txt"
    # )
    
    # Example of creating RAG documents from SofaScore and Transfermarkt (moved from scraper's create_rag_documents)
    # These were previously creating .txt files in data/processed, so we'll simulate that here
    # SofaScore Calendar (from scrape_sofascore.py)
    sofascore_matches_path = config.RAW_DATA_PATH / "sofascore_upcoming_matches.json"
    if sofascore_matches_path.exists():
        with open(sofascore_matches_path, 'r', encoding='utf-8') as f:
            matches = json.load(f)
        rag_text = "CALENDRIER CAN 2025\n\n"
        for match in matches['matches']:
            rag_text += f"{match['date']}: {match['home_team']} vs {match['away_team']}\n"
        with open(config.PROCESSED_DATA_PATH / "sofascore_calendar.txt", 'w', encoding='utf-8') as f:
            f.write(rag_text)
        logger.info(f"SofaScore calendar processed to {config.PROCESSED_DATA_PATH / 'sofascore_calendar.txt'}")

    # Transfermarkt Teams/Players (from scrape_transfermarkt.py)
    transfermarkt_teams_path = config.RAW_DATA_PATH / "transfermarkt_african_teams_with_players.json"
    if transfermarkt_teams_path.exists():
        with open(transfermarkt_teams_path, 'r', encoding='utf-8') as f:
            teams = json.load(f)
        rag_text = "EQUIPES ET JOUEURS CAN 2025\n\n"
        for team in teams:
            rag_text += f"EQUIPE: {team['name']}\n"
            rag_text += "JOUEURS:\n"
            for player in team.get('players', []):
                rag_text += f"- {player['name']} ({player['position']})\n"
            rag_text += "\n"
        with open(config.PROCESSED_DATA_PATH / "transfermarkt_teams_players.txt", 'w', encoding='utf-8') as f:
            f.write(rag_text)
        logger.info(f"Transfermarkt teams processed to {config.PROCESSED_DATA_PATH / 'transfermarkt_teams_players.txt'}")

    # Wikipedia data (from scrape_wikipedia.py)
    wikipedia_history_path = config.RAW_DATA_PATH / "wikipedia_can_history.json"
    wikipedia_editions_path = config.RAW_DATA_PATH / "wikipedia_can_editions.json"
    rag_text = ""
    if wikipedia_history_path.exists():
        with open(wikipedia_history_path, 'r', encoding='utf-8') as f:
            history = json.load(f)
        rag_text += f"HISTORIQUE DE LA COUPE D'AFRIQUE DES NATIONS\n\n"
        rag_text += f"{history['summary']}\n\n"
        rag_text += f"Source: {history['url']}\n"
        rag_text += f"Derniere mise a jour: {history['collected_at']}\n\n"
        rag_text += f"{history['full_text']}\n\n"
    if wikipedia_editions_path.exists():
        with open(wikipedia_editions_path, 'r', encoding='utf-8') as f:
            editions = json.load(f)
        for edition in editions:
            rag_text += f"EDITION {edition['year']}\n\n"
            rag_text += f"Titre: {edition['title']}\n"
            rag_text += f"URL: {edition['url']}\n\n"
            rag_text += f"{edition['summary']}\n\n"
            rag_text += f"{edition['full_text']}\n\n"
    if rag_text:
        with open(config.PROCESSED_DATA_PATH / "wikipedia_can_context.txt", 'w', encoding='utf-8') as f:
            f.write(rag_text)
        logger.info(f"Wikipedia data processed to {config.PROCESSED_DATA_PATH / 'wikipedia_can_context.txt'}")

    # Create master RAG document from processed files (using processor.create_master_rag_document)
    processor.create_master_rag_document(
        processed_data_dir=config.PROCESSED_DATA_PATH,
        output_filepath=config.CORPUS_PATH / "master_can_2025_rag.txt"
    )

    # Note: `append_squad_list` should be called if you have a static squad list to append.
    # For now, this is a placeholder. You'd define `squad_list_content` here or load it.
    # processor.append_squad_list_to_file(
    #     filepath=config.CORPUS_PATH / "can_2025_squad_lists_rag.txt",
    #     squad_list_content="""Your static squad list content here"""
    # )
    
    # Merge and deduplicate the final corpus (optional, but good practice if intermediate files were also created)
    processor.merge_and_deduplicate_rag_corpus(
        input_filepath=config.CORPUS_PATH / "master_can_2025_rag.txt",
        output_filepath=config.CORPUS_PATH / "final_can_2025_rag_corpus.txt"
    )

    logger.info("Data processing complete.")

    # --- Step 3: Ingest into Vector Store ---
    logger.info("Step 3: Ingesting documents into the vector store...")
    loader.ingest_pipeline()
    logger.info("Vector store ingestion complete.")

    logger.info("Data ingestion pipeline finished successfully!")

if __name__ == "__main__":
    main()